{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Camera Equipment Data\n",
    "\n",
    "Sadly, the metadata associated the camera and lens that took each photo isn't included in the dataset I downloaded from Macaulay. However, I can scrape the data from the Macaulay website. Since it's only ~3,000 photos that I'm interested in, I shouldn't raise any red flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catalog_number</th>\n",
       "      <th>common_name</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>photographer</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>rating</th>\n",
       "      <th>num_ratings</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>427148171</td>\n",
       "      <td>Blue-rumped Parrot</td>\n",
       "      <td>Psittinus cyanurus</td>\n",
       "      <td>Saravanan Krishnamurthy</td>\n",
       "      <td>2022-03-19</td>\n",
       "      <td>2022</td>\n",
       "      <td>5.0</td>\n",
       "      <td>37</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>522492561</td>\n",
       "      <td>Bornean Banded-Pitta</td>\n",
       "      <td>Hydrornis schwaneri</td>\n",
       "      <td>Wai Loon Wong</td>\n",
       "      <td>2022-12-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>5.0</td>\n",
       "      <td>21</td>\n",
       "      <td>Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>519807861</td>\n",
       "      <td>Surf Scoter</td>\n",
       "      <td>Melanitta perspicillata</td>\n",
       "      <td>Frank Lin</td>\n",
       "      <td>2023-01-02</td>\n",
       "      <td>2023</td>\n",
       "      <td>5.0</td>\n",
       "      <td>54</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>584587231</td>\n",
       "      <td>Black-throated Gray Warbler</td>\n",
       "      <td>Setophaga nigrescens</td>\n",
       "      <td>Joachim Bertrands | Ornis Birding Expeditions</td>\n",
       "      <td>2023-06-13</td>\n",
       "      <td>2023</td>\n",
       "      <td>5.0</td>\n",
       "      <td>54</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>625632330</td>\n",
       "      <td>Scissor-tailed Flycatcher</td>\n",
       "      <td>Tyrannus forficatus</td>\n",
       "      <td>Matt Zuro</td>\n",
       "      <td>2024-10-30</td>\n",
       "      <td>2024</td>\n",
       "      <td>5.0</td>\n",
       "      <td>54</td>\n",
       "      <td>North America</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   catalog_number                  common_name          scientific_name  \\\n",
       "0       427148171           Blue-rumped Parrot       Psittinus cyanurus   \n",
       "1       522492561         Bornean Banded-Pitta      Hydrornis schwaneri   \n",
       "2       519807861                  Surf Scoter  Melanitta perspicillata   \n",
       "3       584587231  Black-throated Gray Warbler     Setophaga nigrescens   \n",
       "4       625632330    Scissor-tailed Flycatcher      Tyrannus forficatus   \n",
       "\n",
       "                                    photographer        date  year  rating  \\\n",
       "0                        Saravanan Krishnamurthy  2022-03-19  2022     5.0   \n",
       "1                                  Wai Loon Wong  2022-12-21  2022     5.0   \n",
       "2                                      Frank Lin  2023-01-02  2023     5.0   \n",
       "3  Joachim Bertrands | Ornis Birding Expeditions  2023-06-13  2023     5.0   \n",
       "4                                      Matt Zuro  2024-10-30  2024     5.0   \n",
       "\n",
       "   num_ratings       location  \n",
       "0           37           Asia  \n",
       "1           21           Asia  \n",
       "2           54  North America  \n",
       "3           54  North America  \n",
       "4           54  North America  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_records_df = pd.read_csv(\"../results/top_records.csv\")\n",
    "top_records_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_macaulay_technical_info(catalog_numbers, base_url=\"https://macaulaylibrary.org/asset/{}\", delay=1.0):\n",
    "    \"\"\"\n",
    "    Scrape technical information from Macaulay Library pages.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    catalog_numbers : list\n",
    "        List of catalog numbers to scrape (e.g., [\"123456\", \"789012\"])\n",
    "    base_url : str, optional\n",
    "        Template URL with {} placeholder for catalog number\n",
    "        Default: \"https://macaulaylibrary.org/asset/{}\"\n",
    "    delay : float, optional\n",
    "        Time in seconds to wait between requests\n",
    "        Default: 1.0\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing technical information\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Research Bot (Responsible scraping for academic research)'\n",
    "    })\n",
    "    \n",
    "    for catalog_number in catalog_numbers:\n",
    "        try:\n",
    "            # Make request\n",
    "            url = base_url.format(catalog_number)\n",
    "            response = session.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find the \"Technical information\" heading\n",
    "            tech_heading = soup.find('h3', string='Technical information')\n",
    "            if not tech_heading:\n",
    "                print(f\"No technical information heading found for {catalog_number}\")\n",
    "                continue\n",
    "                \n",
    "            # Get the definition list that follows the heading\n",
    "            tech_section = tech_heading.find_next('dl', class_='defList')\n",
    "            \n",
    "            if tech_section:\n",
    "                # Extract information\n",
    "                tech_info = {'catalog_number': catalog_number}\n",
    "                terms = tech_section.find_all('dt')\n",
    "                definitions = tech_section.find_all('dd')\n",
    "                \n",
    "                for term, definition in zip(terms, definitions):\n",
    "                    column_name = term.text.strip().lower().replace(' ', '_')\n",
    "                    tech_info[column_name] = definition.text.strip()\n",
    "                \n",
    "                all_data.append(tech_info)\n",
    "            \n",
    "            # Respect the site by waiting between requests\n",
    "            time.sleep(delay)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {catalog_number}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "def scrape_in_batches(catalog_numbers, batch_size=500, delay=1.0, checkpoint_dir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    Scrape data in batches with checkpointing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    catalog_numbers : list\n",
    "        List of all catalog numbers to process\n",
    "    batch_size : int, optional\n",
    "        Number of items to process per batch (default: 500)\n",
    "    delay : float, optional\n",
    "        Delay between requests in seconds (default: 1.0)\n",
    "    checkpoint_dir : str, optional\n",
    "        Directory to save checkpoint files (default: \"checkpoints\")\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Combined results from all batches\n",
    "    \"\"\"\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Timestamp for this run\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    results_list = []\n",
    "    total_batches = len(catalog_numbers) // batch_size + (1 if len(catalog_numbers) % batch_size else 0)\n",
    "    \n",
    "    for i in range(0, len(catalog_numbers), batch_size):\n",
    "        batch_num = i // batch_size + 1\n",
    "        batch = catalog_numbers[i:i + batch_size]\n",
    "        \n",
    "        print(f\"\\nStarting batch {batch_num} of {total_batches}\")\n",
    "        print(f\"Processing catalog numbers {i} to {i + len(batch)}\")\n",
    "        \n",
    "        # Scrape batch\n",
    "        batch_results = scrape_macaulay_technical_info(batch, delay=delay)\n",
    "        results_list.append(batch_results)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = os.path.join(\n",
    "            checkpoint_dir, \n",
    "            f'macaulay_results_{timestamp}_batch_{batch_num}_of_{total_batches}.csv'\n",
    "        )\n",
    "        batch_results.to_csv(checkpoint_path, index=False)\n",
    "        print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "        \n",
    "        # Save combined results so far\n",
    "        combined_path = os.path.join(\n",
    "            checkpoint_dir,\n",
    "            f'macaulay_results_{timestamp}_combined.csv'\n",
    "        )\n",
    "        pd.concat(results_list).to_csv(combined_path, index=False)\n",
    "        print(f\"Updated combined results: {combined_path}\")\n",
    "        \n",
    "        if batch_num < total_batches:\n",
    "            print(f\"Taking a 30 second break before next batch...\")\n",
    "            time.sleep(30)\n",
    "    \n",
    "    return pd.concat(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraping process...\n",
      "Total catalog numbers: 3203\n",
      "Current time: 2024-12-30 21:20:07\n",
      "\n",
      "Starting batch 1 of 7\n",
      "Processing catalog numbers 0 to 500\n",
      "Saved checkpoint: macaulay_checkpoints/macaulay_results_20241230_212007_batch_1_of_7.csv\n",
      "Updated combined results: macaulay_checkpoints/macaulay_results_20241230_212007_combined.csv\n",
      "Taking a 30 second break before next batch...\n",
      "\n",
      "Starting batch 2 of 7\n",
      "Processing catalog numbers 500 to 1000\n",
      "Saved checkpoint: macaulay_checkpoints/macaulay_results_20241230_212007_batch_2_of_7.csv\n",
      "Updated combined results: macaulay_checkpoints/macaulay_results_20241230_212007_combined.csv\n",
      "Taking a 30 second break before next batch...\n",
      "\n",
      "Starting batch 3 of 7\n",
      "Processing catalog numbers 1000 to 1500\n",
      "Saved checkpoint: macaulay_checkpoints/macaulay_results_20241230_212007_batch_3_of_7.csv\n",
      "Updated combined results: macaulay_checkpoints/macaulay_results_20241230_212007_combined.csv\n",
      "Taking a 30 second break before next batch...\n",
      "\n",
      "Starting batch 4 of 7\n",
      "Processing catalog numbers 1500 to 2000\n",
      "Saved checkpoint: macaulay_checkpoints/macaulay_results_20241230_212007_batch_4_of_7.csv\n",
      "Updated combined results: macaulay_checkpoints/macaulay_results_20241230_212007_combined.csv\n",
      "Taking a 30 second break before next batch...\n",
      "\n",
      "Starting batch 5 of 7\n",
      "Processing catalog numbers 2000 to 2500\n",
      "Saved checkpoint: macaulay_checkpoints/macaulay_results_20241230_212007_batch_5_of_7.csv\n",
      "Updated combined results: macaulay_checkpoints/macaulay_results_20241230_212007_combined.csv\n",
      "Taking a 30 second break before next batch...\n",
      "\n",
      "Starting batch 6 of 7\n",
      "Processing catalog numbers 2500 to 3000\n",
      "Saved checkpoint: macaulay_checkpoints/macaulay_results_20241230_212007_batch_6_of_7.csv\n",
      "Updated combined results: macaulay_checkpoints/macaulay_results_20241230_212007_combined.csv\n",
      "Taking a 30 second break before next batch...\n",
      "\n",
      "Starting batch 7 of 7\n",
      "Processing catalog numbers 3000 to 3203\n",
      "Saved checkpoint: macaulay_checkpoints/macaulay_results_20241230_212007_batch_7_of_7.csv\n",
      "Updated combined results: macaulay_checkpoints/macaulay_results_20241230_212007_combined.csv\n",
      "\n",
      "Scraping completed!\n"
     ]
    }
   ],
   "source": [
    "catalog_numbers = top_records_df['catalog_number'].tolist()\n",
    "print(\"Starting scraping process...\")\n",
    "print(f\"Total catalog numbers: {len(catalog_numbers)}\")\n",
    "print(f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Run the scraper with batching\n",
    "results = scrape_in_batches(\n",
    "    catalog_numbers,\n",
    "    batch_size=500,    # Process 500 at a time\n",
    "    delay=1.0,         # 1 second between requests\n",
    "    checkpoint_dir=\"macaulay_checkpoints\"\n",
    ")\n",
    "\n",
    "print(\"\\nScraping completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes roughly 1.5 hours to run. I could make it run faster, but I want to be careful not to spam the Macaulay website.\n",
    "\n",
    "Now, lets do a final bit of processing and save the results. We don't need to keep rows that are missing the camera information. We also want to join the results back to the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 487 records with missing camera model information and 2716 records with camera model information.\n"
     ]
    }
   ],
   "source": [
    "filtered_results = results[results['model'].notnull()]\n",
    "print(f\"There are {len(results)-len(filtered_results)} records with missing camera model information and {len(filtered_results)} records with camera model information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_results = filtered_results.merge(top_records_df, on='catalog_number', how='left')\n",
    "joined_results.to_csv(\"../results/camera_specs.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web-scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
