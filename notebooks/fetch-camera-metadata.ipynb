{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Camera Metadata\n",
    "\n",
    "Sadly, the metadata associated the camera and lens that took each photo isn't included in the dataset I downloaded from Macaulay. However, I can scrape the data from the Macaulay website. Since it's only ~3,000 photos that I'm interested in, I shouldn't raise any red flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>catalog_number</th>\n",
       "      <th>common_name</th>\n",
       "      <th>scientific_name</th>\n",
       "      <th>photographer</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>rating</th>\n",
       "      <th>num_ratings</th>\n",
       "      <th>location</th>\n",
       "      <th>normalized_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>418857311</td>\n",
       "      <td>Marbled Duck</td>\n",
       "      <td>Marmaronetta angustirostris</td>\n",
       "      <td>David Fernández</td>\n",
       "      <td>2022-02-20</td>\n",
       "      <td>2022</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25</td>\n",
       "      <td>Europe</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>144333121</td>\n",
       "      <td>Crescent-chested Warbler</td>\n",
       "      <td>Oreothlypis superciliosa</td>\n",
       "      <td>Patrick Van Thull</td>\n",
       "      <td>2019-03-08</td>\n",
       "      <td>2019</td>\n",
       "      <td>5.0</td>\n",
       "      <td>31</td>\n",
       "      <td>North America</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>622728122</td>\n",
       "      <td>Coppersmith Barbet</td>\n",
       "      <td>Psilopogon haemacephalus</td>\n",
       "      <td>John Russell</td>\n",
       "      <td>2024-08-20</td>\n",
       "      <td>2024</td>\n",
       "      <td>5.0</td>\n",
       "      <td>62</td>\n",
       "      <td>Asia</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>512323911</td>\n",
       "      <td>Yellow-rumped Cacique</td>\n",
       "      <td>Cacicus cela</td>\n",
       "      <td>Charlie Bostwick</td>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>2022</td>\n",
       "      <td>5.0</td>\n",
       "      <td>27</td>\n",
       "      <td>South America</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>205901211</td>\n",
       "      <td>Yellow-throated Bunting</td>\n",
       "      <td>Emberiza elegans</td>\n",
       "      <td>Sicheng Wan</td>\n",
       "      <td>2018-01-26</td>\n",
       "      <td>2018</td>\n",
       "      <td>5.0</td>\n",
       "      <td>22</td>\n",
       "      <td>Asia</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   catalog_number               common_name              scientific_name  \\\n",
       "0       418857311              Marbled Duck  Marmaronetta angustirostris   \n",
       "1       144333121  Crescent-chested Warbler     Oreothlypis superciliosa   \n",
       "2       622728122        Coppersmith Barbet     Psilopogon haemacephalus   \n",
       "3       512323911     Yellow-rumped Cacique                 Cacicus cela   \n",
       "4       205901211   Yellow-throated Bunting             Emberiza elegans   \n",
       "\n",
       "        photographer        date  year  rating  num_ratings       location  \\\n",
       "0    David Fernández  2022-02-20  2022     5.0           25         Europe   \n",
       "1  Patrick Van Thull  2019-03-08  2019     5.0           31  North America   \n",
       "2       John Russell  2024-08-20  2024     5.0           62           Asia   \n",
       "3   Charlie Bostwick  2022-01-06  2022     5.0           27  South America   \n",
       "4        Sicheng Wan  2018-01-26  2018     5.0           22           Asia   \n",
       "\n",
       "   normalized_rating  \n",
       "0                5.0  \n",
       "1                5.0  \n",
       "2                5.0  \n",
       "3                5.0  \n",
       "4                5.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_records_df = pd.read_csv(\"../results/top_photographs.csv\")\n",
    "top_records_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_macaulay_technical_info(catalog_numbers, base_url=\"https://macaulaylibrary.org/asset/{}\", delay=1.0):\n",
    "    \"\"\"\n",
    "    Scrape technical information from Macaulay Library pages.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    catalog_numbers : list\n",
    "        List of catalog numbers to scrape (e.g., [\"123456\", \"789012\"])\n",
    "    base_url : str, optional\n",
    "        Template URL with {} placeholder for catalog number\n",
    "        Default: \"https://macaulaylibrary.org/asset/{}\"\n",
    "    delay : float, optional\n",
    "        Time in seconds to wait between requests\n",
    "        Default: 1.0\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        DataFrame containing technical information\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        'User-Agent': 'Research Bot (Responsible scraping for academic research)'\n",
    "    })\n",
    "    \n",
    "    for catalog_number in catalog_numbers:\n",
    "        try:\n",
    "            # Make request\n",
    "            url = base_url.format(catalog_number)\n",
    "            response = session.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Find the \"Technical information\" heading\n",
    "            tech_heading = soup.find('h3', string='Technical information')\n",
    "            if not tech_heading:\n",
    "                print(f\"No technical information heading found for {catalog_number}\")\n",
    "                continue\n",
    "                \n",
    "            # Get the definition list that follows the heading\n",
    "            tech_section = tech_heading.find_next('dl', class_='defList')\n",
    "            \n",
    "            if tech_section:\n",
    "                # Extract information\n",
    "                tech_info = {'catalog_number': catalog_number}\n",
    "                terms = tech_section.find_all('dt')\n",
    "                definitions = tech_section.find_all('dd')\n",
    "                \n",
    "                for term, definition in zip(terms, definitions):\n",
    "                    column_name = term.text.strip().lower().replace(' ', '_')\n",
    "                    tech_info[column_name] = definition.text.strip()\n",
    "                \n",
    "                all_data.append(tech_info)\n",
    "            \n",
    "            # Respect the site by waiting between requests\n",
    "            time.sleep(delay)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {catalog_number}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(all_data)\n",
    "\n",
    "def scrape_in_batches(catalog_numbers, batch_size=500, delay=1.0, checkpoint_dir=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    Scrape data in batches with checkpointing.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    catalog_numbers : list\n",
    "        List of all catalog numbers to process\n",
    "    batch_size : int, optional\n",
    "        Number of items to process per batch (default: 500)\n",
    "    delay : float, optional\n",
    "        Delay between requests in seconds (default: 1.0)\n",
    "    checkpoint_dir : str, optional\n",
    "        Directory to save checkpoint files (default: \"checkpoints\")\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Combined results from all batches\n",
    "    \"\"\"\n",
    "    # Create checkpoint directory if it doesn't exist\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Timestamp for this run\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    results_list = []\n",
    "    total_batches = len(catalog_numbers) // batch_size + (1 if len(catalog_numbers) % batch_size else 0)\n",
    "    \n",
    "    for i in range(0, len(catalog_numbers), batch_size):\n",
    "        batch_num = i // batch_size + 1\n",
    "        batch = catalog_numbers[i:i + batch_size]\n",
    "        \n",
    "        print(f\"\\nStarting batch {batch_num} of {total_batches}\")\n",
    "        print(f\"Processing catalog numbers {i} to {i + len(batch)}\")\n",
    "        \n",
    "        # Scrape batch\n",
    "        batch_results = scrape_macaulay_technical_info(batch, delay=delay)\n",
    "        results_list.append(batch_results)\n",
    "        \n",
    "        # Save checkpoint\n",
    "        checkpoint_path = os.path.join(\n",
    "            checkpoint_dir, \n",
    "            f'macaulay_results_{timestamp}_batch_{batch_num}_of_{total_batches}.csv'\n",
    "        )\n",
    "        batch_results.to_csv(checkpoint_path, index=False)\n",
    "        print(f\"Saved checkpoint: {checkpoint_path}\")\n",
    "        \n",
    "        # Save combined results so far\n",
    "        combined_path = os.path.join(\n",
    "            checkpoint_dir,\n",
    "            f'macaulay_results_{timestamp}_combined.csv'\n",
    "        )\n",
    "        pd.concat(results_list).to_csv(combined_path, index=False)\n",
    "        print(f\"Updated combined results: {combined_path}\")\n",
    "        \n",
    "        if batch_num < total_batches:\n",
    "            print(f\"Taking a 10 second break before next batch...\")\n",
    "            time.sleep(10)\n",
    "    \n",
    "    return pd.concat(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting scraping process...\n",
      "Total catalog numbers: 3292\n",
      "Current time: 2024-12-31 19:29:54\n",
      "\n",
      "Starting batch 1 of 14\n",
      "Processing catalog numbers 0 to 250\n",
      "Saved checkpoint: ../results/checkpoints/macaulay_results_20241231_192954_batch_1_of_14.csv\n",
      "Updated combined results: ../results/checkpoints/macaulay_results_20241231_192954_combined.csv\n",
      "Taking a 10 second break before next batch...\n",
      "\n",
      "Starting batch 2 of 14\n",
      "Processing catalog numbers 250 to 500\n",
      "Saved checkpoint: ../results/checkpoints/macaulay_results_20241231_192954_batch_2_of_14.csv\n",
      "Updated combined results: ../results/checkpoints/macaulay_results_20241231_192954_combined.csv\n",
      "Taking a 10 second break before next batch...\n",
      "\n",
      "Starting batch 3 of 14\n",
      "Processing catalog numbers 500 to 750\n",
      "Saved checkpoint: ../results/checkpoints/macaulay_results_20241231_192954_batch_3_of_14.csv\n",
      "Updated combined results: ../results/checkpoints/macaulay_results_20241231_192954_combined.csv\n",
      "Taking a 10 second break before next batch...\n",
      "\n",
      "Starting batch 4 of 14\n",
      "Processing catalog numbers 750 to 1000\n",
      "Saved checkpoint: ../results/checkpoints/macaulay_results_20241231_192954_batch_4_of_14.csv\n",
      "Updated combined results: ../results/checkpoints/macaulay_results_20241231_192954_combined.csv\n",
      "Taking a 10 second break before next batch...\n",
      "\n",
      "Starting batch 5 of 14\n",
      "Processing catalog numbers 1000 to 1250\n",
      "Saved checkpoint: ../results/checkpoints/macaulay_results_20241231_192954_batch_5_of_14.csv\n",
      "Updated combined results: ../results/checkpoints/macaulay_results_20241231_192954_combined.csv\n",
      "Taking a 10 second break before next batch...\n",
      "\n",
      "Starting batch 6 of 14\n",
      "Processing catalog numbers 1250 to 1500\n",
      "Saved checkpoint: ../results/checkpoints/macaulay_results_20241231_192954_batch_6_of_14.csv\n",
      "Updated combined results: ../results/checkpoints/macaulay_results_20241231_192954_combined.csv\n",
      "Taking a 10 second break before next batch...\n",
      "\n",
      "Starting batch 7 of 14\n",
      "Processing catalog numbers 1500 to 1750\n",
      "Saved checkpoint: ../results/checkpoints/macaulay_results_20241231_192954_batch_7_of_14.csv\n",
      "Updated combined results: ../results/checkpoints/macaulay_results_20241231_192954_combined.csv\n",
      "Taking a 10 second break before next batch...\n",
      "\n",
      "Starting batch 8 of 14\n",
      "Processing catalog numbers 1750 to 2000\n",
      "Saved checkpoint: ../results/checkpoints/macaulay_results_20241231_192954_batch_8_of_14.csv\n",
      "Updated combined results: ../results/checkpoints/macaulay_results_20241231_192954_combined.csv\n",
      "Taking a 10 second break before next batch...\n",
      "\n",
      "Starting batch 9 of 14\n",
      "Processing catalog numbers 2000 to 2250\n",
      "Saved checkpoint: ../results/checkpoints/macaulay_results_20241231_192954_batch_9_of_14.csv\n",
      "Updated combined results: ../results/checkpoints/macaulay_results_20241231_192954_combined.csv\n",
      "Taking a 10 second break before next batch...\n",
      "\n",
      "Starting batch 10 of 14\n",
      "Processing catalog numbers 2250 to 2500\n",
      "Saved checkpoint: ../results/checkpoints/macaulay_results_20241231_192954_batch_10_of_14.csv\n",
      "Updated combined results: ../results/checkpoints/macaulay_results_20241231_192954_combined.csv\n",
      "Taking a 10 second break before next batch...\n",
      "\n",
      "Starting batch 11 of 14\n",
      "Processing catalog numbers 2500 to 2750\n",
      "Saved checkpoint: ../results/checkpoints/macaulay_results_20241231_192954_batch_11_of_14.csv\n",
      "Updated combined results: ../results/checkpoints/macaulay_results_20241231_192954_combined.csv\n",
      "Taking a 10 second break before next batch...\n",
      "\n",
      "Starting batch 12 of 14\n",
      "Processing catalog numbers 2750 to 3000\n",
      "Saved checkpoint: ../results/checkpoints/macaulay_results_20241231_192954_batch_12_of_14.csv\n",
      "Updated combined results: ../results/checkpoints/macaulay_results_20241231_192954_combined.csv\n",
      "Taking a 10 second break before next batch...\n",
      "\n",
      "Starting batch 13 of 14\n",
      "Processing catalog numbers 3000 to 3250\n",
      "Saved checkpoint: ../results/checkpoints/macaulay_results_20241231_192954_batch_13_of_14.csv\n",
      "Updated combined results: ../results/checkpoints/macaulay_results_20241231_192954_combined.csv\n",
      "Taking a 10 second break before next batch...\n",
      "\n",
      "Starting batch 14 of 14\n",
      "Processing catalog numbers 3250 to 3292\n",
      "Saved checkpoint: ../results/checkpoints/macaulay_results_20241231_192954_batch_14_of_14.csv\n",
      "Updated combined results: ../results/checkpoints/macaulay_results_20241231_192954_combined.csv\n",
      "\n",
      "Scraping completed!\n"
     ]
    }
   ],
   "source": [
    "catalog_numbers = top_records_df['catalog_number'].tolist()\n",
    "print(\"Starting scraping process...\")\n",
    "print(f\"Total catalog numbers: {len(catalog_numbers)}\")\n",
    "print(f\"Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# Run the scraper with batching\n",
    "results = scrape_in_batches(\n",
    "    catalog_numbers,\n",
    "    batch_size=250,\n",
    "    delay=.25,\n",
    "    checkpoint_dir=\"../results/checkpoints\"\n",
    ")\n",
    "\n",
    "print(\"\\nScraping completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes roughly 1.5 hours to run. I could make it run faster, but I want to be careful not to spam the Macaulay website.\n",
    "\n",
    "Now, lets do a final bit of processing and save the results. We don't need to keep rows that are missing the camera information. We also want to join the results back to the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 497 records with missing camera model information and 2795 records with camera model information.\n"
     ]
    }
   ],
   "source": [
    "filtered_results = results[results['model'].notnull()]\n",
    "print(f\"There are {len(results)-len(filtered_results)} records with missing camera model information and {len(filtered_results)} records with camera model information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_results = filtered_results.merge(top_records_df, on='catalog_number', how='left')\n",
    "joined_results.to_csv(\"../results/camera_metadata.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web-scraping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
